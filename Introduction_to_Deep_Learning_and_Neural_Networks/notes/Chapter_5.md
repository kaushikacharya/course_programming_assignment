# Autoencoders
1. [Generative Learning](#generative-learning)
2. [Basics of Autoencoders](#basics-of-autoencoders)
3. [Variational Autoencoder: Theory](#variational-autoencoder-theory)

## Generative Learning
- ### Discriminative vs generative models
    - Discriminative models:
        - Learn the probability of a label y based on a data point x.
        - In mathematical terms, denoted by $p(y|x)$.
        - We need to learn a mapping between the data and the classes.
    - Generative models:
        - Learn a probability distribution over the data points without external labels.
        - Mathematically formulated as $p(x)$.
    - Conditional generative models:
        - A category of models that try to learn the probability distribution of the data x conditioned on the labels y.
        - Denoted by $p(x|y)$.
    - Bayes' rule:
        - $p(x|y) = (p(y|x)/p(y))*p(x)$
            - Aforementioned model types are somewhat interconnected.
        - This effectively tells us that we can build eacy type of model as a combination of the other types.

- ### Generative models
    - Probability density function $p(x)$
        - This probability density effectively describes the behavior of our training data and enables us to generate novel data by smapling from the distribution.

- ### Latent variable models
    - Latent variable models aim to model the probability distribution with latent variables.
        - Mathematically, data points $x$ that follow a probability distribution $p(x)$ are mapped into latent variables $z$ tyaht follow a distribution $p(z)$.
    - Latent variables:
        - Transformation of the data points into a **continuous lower-dimensional space**.
    - **Intuitively**, latent variables describe or "explain" the data in a simpler way.
    - We can now define five basic terms:
        - **Prior distribution** $p(z)$
            - Models the behavior of the latent variables.
        - **Likelihood** $p(x|z)$
            - Defines how to map latent variables to the data points.
        - **Joint distribution** $p(x,z) = p(x|z) * p(z)$
            - Multiplication of the likelihood and the prior.
            - Essentially describes our model.
        - **Marginal distribution** $p(x)$
            - Distribution of the original data.
            - Ultimate goal of the model.
        - **Posterior distribution** $p(z|x)$
            - Describes the latent variables that can be produced by a specific data point.
    - Let's define two more terms:
        - **Generation**
            - Refers to the process of computing the data point $x$ from the latent variable $z$.
            - In essence, we move from the latent space to the actual data distribution.
            - Mathematically represented by the likelihood $p(x|z)$.
        - **Inference**
            - Refers to the process of finding the latent variable $z$ from the data point x.
            - Mathematically formulated by the posterior distribution $p(z|x)$.
    - If we assume that we somehow know:
        - Likelihood $p(x|z)$
        - Posterior $p(z|x)$
        - Marginal $p(x)$
        - Prior $p(z)$,
        - we can do the following:

- ### Generation
    - To generate a data point, 
        - we can sample $z$ from $p(z)$,
            - $z \sim p(z)$
        - and then sample the data point $x$ from $p(x|z)$
            - $x \sim p(x|z)$

- ### Inference
    - To infer a latent variable,
        - we sample $x$ from $p(x)$,
            - $x \sim p(x)$
        - and then sample $z$ from $p(z|x)$
            - $z \sim p(z|x)$
    
    - Fundamental question of latent variable models:
        - How all those distributions can be found?
            - Answer: Variational Autoencoders (VAE)

## Basics of Autoencoders
- Autoencoders are simple neural networks such that their output is their input.
- Goal:
    - Learn how to reconstruct the input data.
- Additional info (KA):
    - [Variational Autoencoders: A Vanilla Implementation](https://mlarchive.com/deep-learning/variational-autoencoders-a-vanilla-implementation/)
        - Main goal:
            - To generate high quality output data (e.g. images, texts, or sounds) that belong to the same distribution of the input data.
        - Three main families of generative models:
            - Variational autoencoders (VAE)
            - Generative Adversarial Network (GANs)
            - Diffusion Models
- 1st part:
    - Encoder
        - Receives the input and encodes it in a latent space of a lower dimension (the latent variables $z$).
        - We can think of the latent space as a continuous low-dimensional space.
            - ?? continuous: Does it refers to dense vector representation?
- 2nd part:
    - Decoder
        - Takes the low-dimensional vector and decodes it in order to produce the original input.
- Applications of the latent vector $z$ includes:
    - Compression
    - Dimensionality reduction
- We can apply them to entirely novel data. Practical applications include:
    - Data denoising:
        - Feed the network with a noisy image and train them to output the same image but without the noise.
    - Training data augmentation
    - Anomaly detection:
        - Train an autoencoder on data from a single category so that every anomaly gives a large reconstruction error.
- Vanilla autoencoders are trained using a reconstruction loss, which in its simplest form is nothing more than the L2 distance.

- ### Exercise
    - Description of the model i.e. each of the layers is written as comment in the [code](../code/autoencoder.py).
    - Few things to notice:
        - In the first part of the network, the size of the input is gradually decreasing, resulting in a compact latent representation.
        - In the second part, [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) layers are increasing the size with the goal to output the original size on the final layer.
            - Additional info (KA):
                - [Transposed convolution animation](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md#transposed-convolution-animations)
        - Each layer is followed by a ```ReLu``` activation.
        - The final output is passed through a ```Sigmoid```.
    - Additional info (KA):
        - Calculate number of CNN parameters:
            - Explained by the user *hbaderts* in [StackOverflow thread](https://stackoverflow.com/questions/42786717/how-to-calculate-the-number-of-parameters-for-convolutional-neural-network/42787467)
            - Weights: $n*m*k*l$
                - $n * m$: filter size
                - $l$: Number of feature maps as input
                - $k$: Number of feature maps as output
            - Bias: $k$
                - Bias term for each output feature map.
            - Hence total number of parameters:
                - $(n*m*l + 1)*k$
        - Calculation for the exercise:
            - 1st CNN layer:
                - Number of parameters = $(F*F*l + 1)*k$
                    - $(F*F*3 + 1)*12 = 588$
                        - Kernel size: $F*F$
                        - Solving above equation gives the kernel size: $F = 4$
                - Next we need to calculate the parameter values based on the equation defined in [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).
                    - $H_{out} = (H_{in} + 2P - D(F-1) - 1)/S + 1$
                        - $P$: Padding
                        - $D$: Dilation
                        - $S$: Stride
                    - One possible solution:
                        - $P = 1$
                        - $D = 1 (default)$
                        - $S = 2$
            - Similarly parameter values for rest of the CNN layers can be computed.
    - Solution:
        - [Kaushik](../code/autoencoder_exercise.py)
            - Learning:
                - ```ReLU``` layers needs to be explicitly defined as the test cases in the course checks for the network layers.
                    - Simply executing ```torch.relu``` inside ```forward``` function throws error for the test cases.
        - [Official Solution](../code/autoencoder_official_solution.py)
            - Good practice:
                - Encoder and Decoder layers are grouped together using ```nn.Sequential```.

## Variational Autoencoder: Theory
- ### Overview
    - In simple terms, a variational encoder is a probabilistic version of autoencoders.
    - Each latent variable $z$ that is generated from the input represents a probability distribution (posterior distribution denoted by $p(z|x)$).
    - Encoder approximates the posterior by computing another distribution $q(z|x)$, known as the **variational posterior**.
    - A probability distribution is fully characterized by its parameters.
        - So it is enough to pass the parameters of the probability distribution in the decoder instead of simply passing the latent vector $z$ like the simple autoencoder.
    - The decoder receives the distribution parameters and tries to reconstruct the input x.
    - Challenge:
        - One cannot backpropagate through a sampling operation.

- ### Train a variational autoencoder
    - True posterior: $p(z|x)$
    - Variational posterior: $q(z|x)$
        - The course mentions variational posterior as $p(z|x)$ which in my opinion is incorrect as per the statement mentioned few sentences above.
    - Goal:
        - Variational posterior as close as possible to the true posterior.
    - Evidence Lower Bound (ELBO):
        - Additional info (KA):
            - https://mbernste.github.io/posts/elbo/
                - Explains definition, context and derivation.
        - Loss = Reconstruction term - KL Divergence
            - 1st term: Reconstruction term
                - Controls how well the VAE reconstructs a data point $x$ from a sample $z$ of the variational posterior.
                - Known as **negative reconstruction error**.
                - If data points are binary (follow the Bernoulli distribution):
                    - reconstruction term can be proved to:
                        - $log\, p_\theta(x_i|z_i) = \sum_{j=1}^n[x_{ij}log\,p_{ij} + (1 - x_{ij})log(1-p_{ij})]$
            - 2nd term: KL Divergence:
                - Controls how close the variational posterior is to the prior.
                - If we assume Gaussian prior distribution:
                    - -(1/2)$\sum_{j=1}^J(1 + log(\sigma_j^2) -\mu_j^2 - \sigma_j^2)$
    - Implementation:
        - In practice, we used closed analytical forms to compute the ELBO.
        - Reconstruction term:
            - When the data points are binary (follow the Bernoulli distribution), the equation is simply the binary cross entropy.
                - Implemented in PyTorch using ```torch.nn.BCELoss(reduction='sum')```
        - KL-Divergence term:
            - If we assume that the prior distribution is a Gaussian, then KL-Divergence also has a closed form.
            - Additional info (KA):
                - [Derivation of KL Divergence for Gaussian distribution](https://leenashekhar.github.io/2019-01-30-KL-Divergence/)
        - Solution:
            - [Kaushik](../code/elbo_exercise.py)
            - [Official Solution](../code/elbo_official_solution.py)

- ### Reparameterization trick
    - Intuition:
        - Because we cannot compute the gradient of an expectation, we want to rewrite the expectation so that the distribution is independent of the parameter $\theta$.
    - Formulating the abstract idea:
        - Transform a sample from a **fixed**, known distribution to a sample from $q_{\phi}(z)$.
        - If we consider the Gaussian distribution, we can express $z$ wrt. a fixed $\epsilon$, where $\epsilon$ follows the normal distribution $N(0,1)$.
        - So now $\epsilon$ is the stochastic term.
    - Backpropagation:
        - Since backpropagation cannot be performed in a fully stochastic operation, hence we will not backpropagate through $\epsilon$.
        - So, instead:
            - A fixed part stochastic with $\epsilon$ is kept.
            - The mean and the standard deviation are trained.
        - Next, backpropagate through the mean $\mu$ and the standard deviation $\sigma$ that are outputs of the encoder.
        - Define the latent space vector $z$ as
            - $z = \mu + \sigma\epsilon$ with $\epsilon$ ~ $N(0,1)$
        - $\epsilon$ term introduces the stochastic part and is **not** involved in the training process.
        - Therefore, we can now compute the gradient and run backpropagation of ELBO w.r.t. variational parameters $\theta$.
    - Exercise:
        - Implementation of reparameterization trick in Pytorch
        - Solution:
            - [Kaushik](../code/reparameterization_trick_exercise.py)
            - [Official Solution](../code/reparameterization_trick_official_soluiton.py)
                - IMHO, sampling equation is incorrect.
                    - $sample = mu + (eps * var)$
                        - Instead of $var$ we should use $std$.
                - Refer [mlarchive's implementation](https://mlarchive.com/deep-learning/variational-autoencoders-a-vanilla-implementation/)
                    - Have a look at the section **Reparameterization Trick**
