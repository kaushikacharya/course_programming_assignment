# Conclusion

1. [Where to go from here?](#where-to-go-from-here)
2. [Appendix](#appendix)

## Where to go from here?

- ### Recap

- ### What's next for deep learning

  - Some topics you might want to look into next:
    - Reinforcement Learning
    - Tensorflow, MXNet, JAX, and other frameworks
    - State-of-the-art Computer Vision architectures such as EfficientNet
    - Natural Language processing systems based on Transformers such as BERT and GPT
    - Self-supervised learning
    - Representation learning
    - DL in autonomous vehicles and AI-assisted driving
    - Medical applications
    - Privacy in AI
    - AutoML
    - MLOps
    - Artificial General Intelligence (AGI)

- ### The path forward

  - You can choose to go down either of the three routes listed below:
    - [Kaggle](https://www.kaggle.com/)
    - [PaperWithCode](https://paperswithcode.com/)
      - For those interested in the research aspect of deep learning:
        - Start reading research papers.
        - Explore their implementation and applications.
    - [AI Summer](https://theaisummer.com/)
      - Implementations of different algorithms.
      - More articles on the intuition and math behind them.
      - Guides on how to build deep-learning applications.

## Appendix

- ### Software

- ### Datasets

- ### Mathematics

  - [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
  - [Min-max games](https://en.wikipedia.org/wiki/Minimax)
  - [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)
  - [Eigenvalues and Eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)
  - [Laplacian matrix](https://en.wikipedia.org/wiki/Laplacian_matrix)
  - [Chebyshev approximation](https://en.wikipedia.org/wiki/Approximation_theory)
