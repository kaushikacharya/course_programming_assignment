{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ud_6CBgR0MH3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mh5Jrd070MH6"
   },
   "source": [
    "# Chapter 5: Going Classy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gp2KjkM0MH6"
   },
   "source": [
    "Now that we've built a class to make training models easier, it's time to put your knowledge into action :-)\n",
    "\n",
    "We're using the same synthetic dataset from the previous challenges (*b = 0.5* and *w = -3* for a **linear regression with a single feature (x)**), but this time you'll be implementing mini-batch gradient descent in PyTorch.\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "y = b + w x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cPXclMy0MH6"
   },
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa0SwK410MH7"
   },
   "outputs": [],
   "source": [
    "true_b = .5\n",
    "true_w = -3\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = (.1 * np.random.randn(N, 1))\n",
    "y = true_b + true_w * x + epsilon\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hq36Mnpb0MH7"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GTsYt280MH7"
   },
   "source": [
    "The preparation of data starts by **converting the data points** from Numpy arrays to PyTorch tensors and sending them to the available **device**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_eatXmx0MH7"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them \n",
    "# into PyTorch's Tensors and then we send them to the \n",
    "# chosen device\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)\n",
    "\n",
    "x_val_tensor = torch.as_tensor(x_val).float().to(device)\n",
    "y_val_tensor = torch.as_tensor(y_val).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tDVZ8zP0MH8"
   },
   "source": [
    "Once again, the data preparation also includes creating datasets and data loaders for both training and validation sets. That's your task now - you're free to choose any mini-batch size you want (and we encourage you to play with different values), but we suggest you to start with 16:\n",
    "\n",
    "Hint: you can use a simple `TensorDataset` for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-M6lix2p0MH8"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAlQt3GT0MH8"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXqgv4GI0MH8"
   },
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWZCOuVP0MH9"
   },
   "source": [
    "Your task is, once again, to define a **model**, an **optimizer**, and a **loss function** to tackle our **linear** regression with a **single input** and **single output**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qmkStMc0MH9"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aM74Zd1m0MH9"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsKhSu0d0MH9"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTMgbfR40MH9"
   },
   "source": [
    "For the model training, we'll use our newly created `StepByStep` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwDpgmd30MH-"
   },
   "outputs": [],
   "source": [
    "class StepByStep(object):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes \n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "        \n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        # Creates the train_step function for our model, \n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step = self._make_train_step()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step = self._make_val_step()\n",
    "\n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to define a SummaryWriter to interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter('{}/{}_{}'.format(folder, name, suffix))\n",
    "\n",
    "    def _make_train_step(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "        \n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step\n",
    "    \n",
    "    def _make_val_step(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step\n",
    "            \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and \n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step = self.val_step\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step = self.train_step\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "            \n",
    "        # Once the data loader and step function, this is the same\n",
    "        # mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False    \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def train(self, n_epochs, seed=42):\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            # If a SummaryWriter has been set...\n",
    "            if self.writer:\n",
    "                scalars = {'training': loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation': val_loss})\n",
    "                # Records both losses for each epoch under the main tag \"loss\"\n",
    "                self.writer.add_scalars(main_tag='loss',\n",
    "                                        tag_scalar_dict=scalars,\n",
    "                                        global_step=epoch)\n",
    "\n",
    "        if self.writer:\n",
    "            # Closes the writer\n",
    "            self.writer.close()\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.total_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training   \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval() \n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def add_graph(self):\n",
    "        # Fetches a single mini-batch so we can use add_graph\n",
    "        if self.train_loader and self.writer:\n",
    "            x_sample, y_sample = next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model, x_sample.to(self.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjXGx9Jg0MIA"
   },
   "source": [
    "Your task is to create an instance of the `StepByStep` class using the model, loss, and optimizer created in the \"Model Configuration\", set the data loaders, train for 200 epochs, and plot the losses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30_eCgwF0MIA"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MA1ZQmeT0MIA"
   },
   "outputs": [],
   "source": [
    "sbs = StepByStep(model, loss_fn, optimizer)\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "sbs.train(n_epochs=200)\n",
    "\n",
    "fig = sbs.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SuNXBwa0MIC"
   },
   "source": [
    "Congratulations! You successfully trained a PyTorch model using just a handful of lines of code and the `StepByStep` class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLJkyrMa0MIC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Challenges04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
