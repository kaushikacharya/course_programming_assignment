{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gortzLTozZiQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaCbvlkvzZiT"
   },
   "source": [
    "# Chapter 2: Visualizing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7e6X_Q8zZiT"
   },
   "source": [
    "Now that you've learned how gradient descent works, it's time to put your knowledge into action :-)\n",
    "\n",
    "We're generating a new synthetic dataset using *b = 0.5* and *w = -3* for a **linear regression with a single feature (x)**:\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "y = b + w x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYZD8-cWzZiU"
   },
   "source": [
    "You'll implement the **five steps** of gradient descent in order to **learn these parameters** from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiLmfvcUzZiU"
   },
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6CUv9nmxzZiU"
   },
   "outputs": [],
   "source": [
    "true_b = .5\n",
    "true_w = -3\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "epsilon = (.1 * np.random.randn(N, 1))\n",
    "y = true_b + true_w * x + epsilon\n",
    "\n",
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-umPOJ0zZiV"
   },
   "source": [
    "## Step 0: Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_PfUGPEzZiV"
   },
   "source": [
    "The first step - actually, the zeroth step - is the *random initialization* of the parameters. Using Numpy's `random.randn` method, you should write code to initialize both *b* and *w*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OA_VYCTkzZiW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37454012] [0.95071431]\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "\n",
    "b = np.random.rand(1)\n",
    "w = np.random.rand(1)\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xu6EZc4YzZiX"
   },
   "source": [
    "## Step 1: Compute Model's Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NL2nSNIYzZiX"
   },
   "source": [
    "The first step (for real) is the **forward pass**, that is, the **predictions** of the model. Our model is a linear regression with a single feature (x), and its parameters are *b* and *w*. You should write code to generate predictions (yhat):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PsZ9yJ_BzZiX"
   },
   "outputs": [],
   "source": [
    "# Step 1 - Computes our model's predicted output - forward pass\n",
    "yhat = b + w*x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHtAF_kCzZiX"
   },
   "source": [
    "## Step 2: Compute the Mean Squared Error (MSE) Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJoIE-g1zZiY"
   },
   "source": [
    "Since our model is a linear regression, the appropriate loss is the **Mean Squared Error (MSE)** loss:\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "error_i = \\hat{y_i} - y_i\n",
    "\\\\\n",
    "\\Large\n",
    "loss = \\frac{1}{N}\\sum_{i=0}^N{error_i^2}\n",
    "$$\n",
    "\n",
    "For each data point (i) in our training set, you should write code to compute the difference between the model's predictions (yhat) and the actual values (y_train), and use the errors of all N data points to compute the loss:\n",
    "\n",
    "Obs.: DO NOT use loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FoBQIegdzZiY"
   },
   "outputs": [],
   "source": [
    "error = yhat - y_train\n",
    "loss = (error**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.98961138]\n",
      " [ 0.05998564]\n",
      " [ 3.09783772]\n",
      " [-0.05270525]\n",
      " [ 2.75726298]\n",
      " [ 0.14384401]\n",
      " [ 0.79165467]\n",
      " [ 1.20061059]\n",
      " [ 1.69884829]\n",
      " [ 3.80414561]\n",
      " [ 0.3601495 ]\n",
      " [ 2.99349823]\n",
      " [ 3.02532782]\n",
      " [ 2.64114613]\n",
      " [ 0.01409053]\n",
      " [ 0.3858017 ]\n",
      " [ 0.51289238]\n",
      " [-0.13567664]\n",
      " [ 3.69526617]\n",
      " [ 2.28962965]\n",
      " [ 2.72210829]\n",
      " [ 2.39173846]\n",
      " [ 3.50974555]\n",
      " [ 2.47541577]\n",
      " [ 0.96168611]\n",
      " [ 0.8178027 ]\n",
      " [ 0.2648168 ]\n",
      " [ 2.71999435]\n",
      " [ 3.63076038]\n",
      " [ 2.08615767]\n",
      " [ 2.16735959]\n",
      " [ 1.90696647]\n",
      " [ 0.58317086]\n",
      " [ 0.97876877]\n",
      " [ 3.67350174]\n",
      " [ 0.66210454]\n",
      " [ 0.50225444]\n",
      " [ 3.34838163]\n",
      " [ 1.34553642]\n",
      " [ 1.17145474]\n",
      " [ 3.07093847]\n",
      " [-0.04377753]\n",
      " [ 3.21625919]\n",
      " [ 1.88786302]\n",
      " [ 3.04915365]\n",
      " [ 3.28332658]\n",
      " [ 0.33037908]\n",
      " [ 0.14516264]\n",
      " [ 2.2311294 ]\n",
      " [ 2.59698542]\n",
      " [ 2.50268721]\n",
      " [-0.10388294]\n",
      " [ 2.33022316]\n",
      " [ 3.06649678]\n",
      " [ 1.58330214]\n",
      " [ 1.08053806]\n",
      " [ 3.01861055]\n",
      " [ 2.90472331]\n",
      " [ 1.94771043]\n",
      " [ 0.69766564]\n",
      " [ 3.66044147]\n",
      " [ 1.02820882]\n",
      " [ 0.39953498]\n",
      " [ 0.9657421 ]\n",
      " [ 2.64208225]\n",
      " [ 0.40374081]\n",
      " [ 1.26142855]\n",
      " [-0.15583773]\n",
      " [ 0.60145317]\n",
      " [ 3.4600949 ]\n",
      " [ 1.8178794 ]\n",
      " [ 1.98046908]\n",
      " [ 0.59203979]\n",
      " [ 0.10578534]\n",
      " [ 0.53077443]\n",
      " [ 3.06729353]\n",
      " [ 1.00483387]\n",
      " [ 3.57626689]\n",
      " [ 1.30095403]\n",
      " [ 1.34538981]]\n"
     ]
    }
   ],
   "source": [
    "# Added by KA\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.495325075114491\n"
     ]
    }
   ],
   "source": [
    "# Added by KA\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyXB8UzNzZiY"
   },
   "source": [
    "## Step 3: Compute the Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyu3RZ92zZiY"
   },
   "source": [
    "PyTorch's autograd will take care of that later on, so we don't have to compute any derivatives yourself! So, no need to manually implement this step.\n",
    "\n",
    "You *still* should understand what the gradients *mean*, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kEP_sc_-zZiZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.464425403927533 2.3835657220659554\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
    "b_grad = 2 * error.mean()\n",
    "w_grad = 2 * (x_train * error).mean()\n",
    "print(b_grad, w_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQvd71bCzZiZ"
   },
   "source": [
    "The gradients above indicate that:\n",
    "- for a tiny increase in the value of the parameter *b*, the loss will increase roughly 2.7 times as much\n",
    "- for a tiny increase in the value of the parameter *w*, the loss will increase roughly 1.8 times as much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCIGnlcUzZiZ"
   },
   "source": [
    "## Step 4: Update the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ8be9COzZiZ"
   },
   "source": [
    "The fourth step is the **parameter update** - you should write code that use the gradients and a learning rate (set to 0.1) to update the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zos1OYZDzZiZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02809758] [0.71235773]\n"
     ]
    }
   ],
   "source": [
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.1\n",
    "\n",
    "# Step 4 - Updates parameters using gradients and the \n",
    "# learning rate\n",
    "b = b - lr*b_grad\n",
    "w = w - lr*w_grad\n",
    "\n",
    "print(b, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-OA87YszZia"
   },
   "source": [
    "## Step 5: Rinse and Repeat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSxqD5LkzZia"
   },
   "source": [
    "The last step consists of putting the other steps together and organize them inside a loop. Write code to fill in the blanks in the loop below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8IjDnvvUzZia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52354035] [-3.03103474]\n",
      "0.008044657695553976\n"
     ]
    }
   ],
   "source": [
    "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
    "np.random.seed(42)\n",
    "\n",
    "b = np.random.rand(1)\n",
    "w = np.random.rand(1)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Step 1: Forward pass\n",
    "    yhat = b + w*x_train\n",
    "    \n",
    "    # Step 2: Compute MSE loss\n",
    "    error = yhat - y_train\n",
    "    loss = (error**2).mean()\n",
    "    \n",
    "    # Step 3: Compute the gradients\n",
    "    b_grad = 2 * error.mean()\n",
    "    w_grad = 2 * (x_train * error).mean()\n",
    "\n",
    "    # Step 4: Update the parameters\n",
    "    b = b - lr*b_grad\n",
    "    w = w - lr*w_grad\n",
    "    \n",
    "print(b, w)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n6-19RxzZia"
   },
   "source": [
    "Congratulations! Your model is able to learn both *b* and *w* that are **really close** to their true values. They will never be a perfect match, though, because of the *noise* we added to the synthetic data (and that's always present in real world data!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6oYp4s0zZib"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Challenges01_question.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7cd38ac66606021aa9ce01b6533275343ec3665f979f0ad4b03360a402e3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
