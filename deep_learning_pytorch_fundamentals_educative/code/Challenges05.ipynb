{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GEn-gdO0O5L"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKqubufT0O5L"
   },
   "source": [
    "# Chapter 6: A Simple Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrrYTyXM0O5M"
   },
   "source": [
    "For the classification, once again, we'll use Scikit-Learnâ€™s make_moons to generate a toy dataset with 100 data points and two features (x1 and x2). We will also add some Gaussian noise but, this time, we're adding **more noise** (0.8 instead of 0.3) to the dataset, thus making it harder for our model to classify the data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olPoWlB30O5M"
   },
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6rMhNYf0O5N"
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.8, random_state=11)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKkl-MEw0O5N"
   },
   "source": [
    "Your first task is to use Scikit-Leanr's `StandardScaler` to standardize our dataset's features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmrtNEUZ0O5N"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBPgSMSA0O5N"
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train = sc.transform(X_train)\n",
    "X_val = sc.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gv1reRTg0O5O"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def figure1(X_train, y_train, X_val, y_val, cm_bright=None):\n",
    "    if cm_bright is None:\n",
    "        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    ax[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)#, edgecolors='k')\n",
    "    ax[0].set_xlabel(r'$X_1$')\n",
    "    ax[0].set_ylabel(r'$X_2$')\n",
    "    ax[0].set_xlim([-2.3, 2.3])\n",
    "    ax[0].set_ylim([-2.3, 2.3])\n",
    "    ax[0].set_title('Generated Data - Train')\n",
    "\n",
    "    ax[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap=cm_bright)#, edgecolors='k')\n",
    "    ax[1].set_xlabel(r'$X_1$')\n",
    "    ax[1].set_ylabel(r'$X_2$')\n",
    "    ax[1].set_xlim([-2.3, 2.3])\n",
    "    ax[1].set_ylim([-2.3, 2.3])\n",
    "    ax[1].set_title('Generated Data - Validation')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kad6ARiE0O5O"
   },
   "outputs": [],
   "source": [
    "fig = figure1(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtqWAMxs0O5P"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhRcTvtJ0O5P"
   },
   "source": [
    "The preparation of data starts by **converting the data points** from Numpy arrays to PyTorch tensors and sending them to the available **device**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NkP0LyB0O5P"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Builds tensors from numpy arrays\n",
    "x_train_tensor = torch.as_tensor(X_train).float()\n",
    "y_train_tensor = torch.as_tensor(y_train.reshape(-1, 1)).float()\n",
    "\n",
    "x_val_tensor = torch.as_tensor(X_val).float()\n",
    "y_val_tensor = torch.as_tensor(y_val.reshape(-1, 1)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2P05TyrR0O5P"
   },
   "source": [
    "Once again, the data preparation also includes creating datasets and data loaders for both training and validation sets. That's your task now - you're free to choose any mini-batch size you want (and we encourage you to play with different values), but we suggest you to start with 16:\n",
    "\n",
    "Hint: you can use a simple `TensorDataset` for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poaPLjFj0O5Q"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-YZTgZR0O5Q"
   },
   "outputs": [],
   "source": [
    "# Builds dataset containing ALL data points\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "# Builds a loader of each set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zl-osz2R0O5Q"
   },
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_blqQpaA0O5Q"
   },
   "source": [
    "Your task is, once again, to define a **model**, an **optimizer**, and a **loss function** to tackle our **linear** regression with a **single input** and **single output**:\n",
    "\n",
    "Hint: your model can either output **logits** or **probabilities** - whatever your choice is, make sure you're using the appropriate loss function for your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY6UJ5F20O5Q"
   },
   "source": [
    "### Answer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xiR2QiI0O5R"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "model = nn.Sequential(nn.Linear(2, 1))\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a BCE loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmA_Aisk0O5R"
   },
   "source": [
    "### Answer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycDN2P1G0O5R"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a BCE loss function\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUofRqOJ0O5R"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzdGd68T0O5R"
   },
   "source": [
    "For the model training, we'll use our `StepByStep` class once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWHOx7Fn0O5S"
   },
   "outputs": [],
   "source": [
    "class StepByStep(object):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes \n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "        \n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        # Creates the train_step function for our model, \n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step = self._make_train_step()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step = self._make_val_step()\n",
    "\n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to define a SummaryWriter to interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter('{}/{}_{}'.format(folder, name, suffix))\n",
    "\n",
    "    def _make_train_step(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "        \n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step\n",
    "    \n",
    "    def _make_val_step(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step\n",
    "            \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and \n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step = self.val_step\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step = self.train_step\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "            \n",
    "        # Once the data loader and step function, this is the same\n",
    "        # mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False    \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def train(self, n_epochs, seed=42):\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            # If a SummaryWriter has been set...\n",
    "            if self.writer:\n",
    "                scalars = {'training': loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation': val_loss})\n",
    "                # Records both losses for each epoch under the main tag \"loss\"\n",
    "                self.writer.add_scalars(main_tag='loss',\n",
    "                                        tag_scalar_dict=scalars,\n",
    "                                        global_step=epoch)\n",
    "\n",
    "        if self.writer:\n",
    "            # Closes the writer\n",
    "            self.writer.close()\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.total_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training   \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval() \n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def add_graph(self):\n",
    "        # Fetches a single mini-batch so we can use add_graph\n",
    "        if self.train_loader and self.writer:\n",
    "            x_sample, y_sample = next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model, x_sample.to(self.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcfaLalR0O5X"
   },
   "source": [
    "Your task is to create an instance of the `StepByStep` class using the model, loss, and optimizer created in the \"Model Configuration\", set the data loaders, train for 40 epochs, and plot the losses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fnvw8MK10O5Z"
   },
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEgut6GM0O5Z"
   },
   "outputs": [],
   "source": [
    "sbs = StepByStep(model, loss_fn, optimizer)\n",
    "sbs.set_loaders(train_loader, val_loader)\n",
    "sbs.train(n_epochs=40)\n",
    "\n",
    "print(sbs.model.state_dict())\n",
    "fig = sbs.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdY1D-qC0O5Z"
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbsEXZvq0O5a"
   },
   "source": [
    "Now, your task is to **predict probabilities** for the first 20 data points in the validation set:\n",
    "    \n",
    "Hint: don't forget to set the model to the proper mode for making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qk6cpxFu0O5a"
   },
   "outputs": [],
   "source": [
    "some_x, some_y = val_dataset[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pwBqMtT0O5a"
   },
   "source": [
    "### Answer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7w-McoT0O5a"
   },
   "outputs": [],
   "source": [
    "# model = nn.Sequential(nn.Linear(2, 1))\n",
    "\n",
    "model.eval()\n",
    "logits = model(some_x.to(device))\n",
    "probs = torch.sigmoid(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEyC-p1u0O5a"
   },
   "source": [
    "### Answer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMZWcsZR0O5a"
   },
   "outputs": [],
   "source": [
    "# model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\n",
    "\n",
    "model.eval()\n",
    "probs = model(some_x.to(device))\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kEz2WOU0O5b"
   },
   "outputs": [],
   "source": [
    "threshold = .5\n",
    "confusion_matrix(some_y, (probs.cpu() >= threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SALg852D0O5b"
   },
   "source": [
    "Congratulations! You successfully trained a PyTorch model for a classification task!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Challenges05.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
